## Chapter 7: Complexity and Effort Scaling

*   **Diseconomy of Scale:** Software effort scales super-linearly with size. Doubling the scope of a system more than doubles the required effort because complexity—driven by the combinatorial explosion of internal interfaces and dependencies—grows faster than the code itself.
*   **Brooks' Law:** Adding more agents to a late project makes it later. New resources require a "context-sync tax," consuming the bandwidth of existing agents for knowledge transfer and increasing the number of coordination paths.
*   **The Impossible Region:** There is a hard limit to schedule compression. Regardless of compute or agent instances applied, certain logical sequences are strictly serial; attempting to crash a schedule beyond this point results in project failure.
*   **Coordination Overhead (O(n²)):** The effort required to maintain consistency grows at the square of the number of participants or modules. Scaling is limited not by individual agent speed, but by the noise of inter-agent communication and state synchronization.
*   **Decomposition as Estimation Strategy:** The accuracy of a complexity estimate is inversely proportional to the size of the task. Meaningful prediction only occurs when a problem is decomposed into small, decoupled units where the unknowns can be isolated.
*   **Three-Point Estimation:** To manage technical uncertainty, use the weighted average of Optimistic (O), Most Likely (M), and Pessimistic (P) scenarios: (O + 4M + P) / 6. This provides a buffer for edge cases and hidden technical debt.
*   **The Fallacy of Interchangeability:** Agent-hours are not fungible with calendar time. Complex logical tasks cannot be partitioned into arbitrary parallel chunks without a significant penalty in integration effort.
*   **The Rayleigh Distribution of Effort:** Effort is never uniform across a lifecycle. It requires a ramp-up for context/architecture, a peak during construction, and a long tail for verification. Forcing a flat effort distribution leads to integration bottlenecks.
*   **The Cone of Uncertainty:** Precision is impossible at the start of a task. The estimate is a range that only narrows as known unknowns are converted into concrete logic; treat early estimates as high-variance hypotheses.
*   **Historical Calibration:** Theoretical models are secondary to historical performance data. An agent's estimation of a task is only valid if calibrated against its own previous completion rates for similar patterns.
*   **Context-Switching Costs:** Significant invisible effort is consumed by re-establishing state. The energy required to load the context can exceed the energy required to execute the actual change.
*   **Size as the Primary Driver:** While tools and processes provide linear improvements, the primary driver of effort remains the volume of logic. Reducing the size of the solution (through abstraction or reuse) is the only way to achieve a sub-linear reduction in effort.
