## Chapter 13: Software Testing

### Fundamentals
*   **Terminology Precision**: Distinguish between **Error** (a human action producing an incorrect result), **Fault** (the manifestation of an error in code; a "bug"), and **Failure** (the inability of a system to perform its required function).
*   **Fault vs. Failure**: A fault is a static defect in the code; a failure is a dynamic event occurring during execution. Not all faults result in failures.
*   **Goal of Testing**: Testing is the process of executing a program with the specific intent of finding faults. A "successful" test is one that uncovers a previously undiscovered fault.
*   **Dijkstra's Dictum**: Software testing can show the presence of bugs, but never their absence.

### White-Box (Structural) Testing
*   **Statement Coverage**: Every source statement executed at least once. Necessary but insufficient.
*   **Branch Coverage (Edge Coverage)**: Every outcome (True/False) from every decision point exercised.
*   **Condition Coverage**: Each individual boolean sub-expression exercised as both True and False.
*   **Multiple Condition Coverage**: All possible combinations of condition outcomes within a single decision tested.
*   **Path Coverage**: Every distinct execution path tested. Mathematically complete but often computationally infeasible.
*   **Data Flow Coverage**: Ensures paths between where a variable is defined (assigned) and where it is used (referenced) are tested.
*   **Loop Testing**: Verify loop boundaries: skip entirely, one pass, two passes, m passes (m < max), and n-1, n, n+1 passes.
*   **Mutation Testing**: Code is intentionally modified ("mutated") to see if existing tests fail. If a test suite cannot "kill" the mutant, the suite is insufficient.

### Black-Box (Functional) Testing
*   **Equivalence Partitioning**: Dividing the input domain into classes from which test cases can be derived. A class represents a set of valid or invalid states for input conditions.
*   **Class Selection**: If a requirement specifies a range (e.g., 1–99), define one valid class (1–99) and two invalid classes (<1 and >99).
*   **Boundary Value Analysis (BVA)**: Focuses on the edges of the input space where faults are most likely to hide.
*   **BVA Implementation**: Test the exact boundary, just below, and just above (e.g., for range 1–99, test 0, 1, 2, 98, 99, 100).
*   **Error Guessing**: Using experience to anticipate fault-prone areas (null pointers, empty lists, divide-by-zero).

### Static Analysis and Review
*   **Inspections**: Formal, structured review where code is examined against a checklist of common fault types (initialization, logic, interface, I/O).
*   **Static Analysis**: Automated identification of potential faults (unreachable code, uninitialized variables, type mismatches) without execution.

### Integration Strategies
*   **Top-Down Integration**: Assembling from the main control module downward. Requires **Stubs** to simulate lower-level modules.
*   **Bottom-Up Integration**: Assembling from atomic modules upward. Requires **Drivers** to pass data to components under test.
*   **Big Bang Integration**: Integrating all components at once. Avoid — makes fault isolation nearly impossible.
*   **Smoke Testing**: Frequent, automated integration test that exercises the breadth of the system to ensure basic stability.

### Regression Testing
*   **Definition**: Re-execution of previously passed tests to ensure that changes have not introduced new faults in existing functionality.
*   **Suite Selection**: Include tests for all software functions, changed components, and components likely affected by the change.
*   **Retest-All vs. Selective**: Running the entire suite is safe but expensive; selective retest uses impact analysis to choose affected tests.

### Test-Driven Development (TDD)
*   **Red-Green-Refactor Cycle**:
    1.  **Red**: Write a small, failing test for a desired piece of functionality.
    2.  **Green**: Write the minimum code to make the test pass.
    3.  **Refactor**: Clean up the code while ensuring the test remains green.
*   **TDD Benefit**: Ensures high test coverage by design and forces consideration of interface and edge cases before implementation.
*   **Test-As-Documentation**: The test suite serves as an executable specification of what the code is supposed to do.

### Practical Testing Principles
*   **Test for the Unexpected**: Prioritize testing invalid, unexpected, and boundary conditions over the happy path.
*   **Independence**: Tests should be independent and idempotent; the outcome of one test should not depend on the state left by another.
*   **Clarity in Failure**: A test failure should clearly indicate what went wrong and where. Generic failure messages are useless for automated recovery.
*   **Automate Everything**: Any test that must be run more than once should be automated.
*   **Test the Test**: Periodically verify that tests can actually fail (e.g., through mutation or temporary code alteration).
*   **Incremental Testing**: Test as you build. Integration of 10 small, tested components is easier than 10 untested components.
*   **Traceability**: Every test case should trace back to a specific requirement or logic branch.
*   **Creative Destruction**: Testing requires a destructive mindset. The goal is to break the system to make it stronger.
*   **Prioritize High-Risk Areas**: Focus testing on complex logic, frequently changed code, and areas with a history of faults.
*   **Maintain Test Code**: Test code is production code. It must be refactored, kept clean, and updated as the system evolves.
*   **The Golden Rule**: Never check in code that does not pass the full local regression suite.
